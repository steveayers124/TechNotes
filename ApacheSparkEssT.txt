ApacheSparkEssT.txt
a fast and general engine for large-scale data processing
began as a design for data science
but now use cases include realtime stream event processing
-SparkSQL -SparkStreaming -MLlib (machine learning) -GraphX


Completed a course on Lynda! Apache Spark Essential Training









---Begin What Spark does well---
Integration, Machine Learning, BI/Analytics, Real-Time Processing, Recommendation Engines
Integration - ETL (Extract, Transform, and Load) to move and standardize data
ML - find hidden patterns in vast data
BI/Analytics - (data science as well) - highest demand, and Spark excels here
Real-Time Processing - fraud detection and realtime user feedback
Recommendation Engines - help users find more value to them in your service

Which languages are people are using with Spark? [DataBricks survey]
* Scala and Python top the list with SQL a close third (then Java, R)
* Over time, may see a shift to SQL, the most universal data language.

High BI/Anal usage reenforces that the basic idea of knowing what's
 going on in with your business drives platform adoption,
 and technology buying decisions.
Close seconds are log processing and fraud detection and analysis.
((1:Business intelligence,
  2:Data Warehousing,
  3:Streaming/RT,
  4:Recommengines,
  5:Log processing,
  6:User facing services,
  7:Fraud Detection and Security))
---End What Spark does well---


---Begin Component overview---
Spark components
Spark Core
* foundation
* Task distribution
* Scheduling
* I/O
SparkSQL
* ANSI standard SQL
* Create/define table structures in Spark first. Then use SQL.
* Allows Tableau and other tools to integrate.
* Uses DataFrames.
 Identical to R DataFrames.
 Similar to Python pandas.
 Like a SQL table.
SparkStreaming
* Streaming analytics
* Micro batches - not true streaming, but its behavior comes close.
* Lambda architecture
 - use historic data to start
 - then as new data comes in, continue to aggregate that data in the result
 - as with Twitter followers, do initial query, but then only update the
    aggregation. Don't re-query. That would just take far too long.
MLlib
* allows the machine learning algorithms to run
* compared to Apache Mahout, which runs on disk instead of in-memory, 9x faster
* many common ML functions, and others from the open source community
GraphX
* graph processing in Spark
* is this traversing a graph data network?
* run graph database jobs by
 - id relationships between entities in your data,
 - then query using those relationships
* this is the in-memory version of Apache Giraph.
* remember, based on dataframes (RDD),
 so its not a true graph database implementation
SparkR
* R statistical package for Spark (2.0.1, new)
* SparkR's distributed dataframes are comparable to data frames in R,
 but employ Sparks method of distributing them on the cluster.
* Integration point for R Studio
---End Component overview---


---Begin brief history of Spark origins---
Modern Data Ecosystems arose in 2004, with Hadoop.
2003, Nutch project development began.
2006, Yahoo hired those developers and released Hadoop as open source.
2006, Google had created a Java interface for working its data called MapReduce.
2008, Facebook provided its analysts and data scientists with an easier way
 of working with data in Hadoop, in the creation of Hive.
So MapReduce is batch oriented, Java based, and slow.
And Hive is SQL abstraction atop MapReduce.
So with the ease of writing queries, batch orientation and slowness persist.
2009, to solve this, some at UC Berkley started a new project to provide easier
 access to big data for data scientists. Spark.
2010, that team open sourced Spark under the BSD license.
2013, the UC Berkley team donated their code to Apache.
2014, Spark became a top-level Apache project. So now Spark has the whole
 community of data professionals supporting and evolving the platform.
---End brief history of Spark origins---


Speed (DAG engine), Ease of use (Spark notebooks, Java, Scala, R, Python, ANSI SQL) [Tableau connects directly using SparkSQL], Generality, Platform agnostic nature
text_file = spark.textFile("hdfs://...")
text_file.flatMap(lambda line: line.split())
  .map(lambda word: (word, 1))
  .reduceByKey(lambda a, b: a+b)
# word count in Spark's Python API, pySpark



.
